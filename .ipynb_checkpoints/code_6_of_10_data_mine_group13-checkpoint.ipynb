{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a903cc6",
   "metadata": {},
   "source": [
    "# Module 5 - Spooky Authorship Identification\n",
    "#### Group 13\n",
    "- Aidan Lonergan\n",
    "- Daniel Lillard\n",
    "- Radhika Garg\n",
    "- Claudine Uwiragiye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de4c9a6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Objective\n",
    "1) Accurately identify the author of the sentences in the test set\n",
    "2) Perform all work with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1cba3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 0 - Import Data\n",
    "1) Create a code notebook called: code_6_of_10_data_mine_group13.ipynb\n",
    "2) Load the dataset into Spark data objects and explore structure, size, and distribution of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0010f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 0 Solution\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Start spark session and load train and test data sets\n",
    "spark = SparkSession.builder.appName(\"Module_5_Project\").getOrCreate()\n",
    "df_train = spark.read.csv('./train.csv', header=True, inferSchema=True, quote='\"', escape='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c498c",
   "metadata": {},
   "source": [
    "##### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print size and descriptive statistics\n",
    "print(\"==== DataSet Shape ====\")\n",
    "print(f\"{len(df_train.columns)} columns\\n{df_train.count()} rows\\n\")\n",
    "\n",
    "print(\"==== DataSet Descriptive Statistics ====\")\n",
    "print(df_train.describe().show())\n",
    "\n",
    "print(\"\\n==== DataSet Unique Authors ====\")\n",
    "print(df_train.select('author').distinct().show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72e21a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 1 - Data Preparation (Exploratory data analysis and text mining pre-processing)\n",
    "1) Perform exploratory data analysis and create visualizations and tables as needed\n",
    "2) Text Preprocessing: perform tasks like tokenization and stopwords removal to clean text data\n",
    "    - Tokenize - split the text into individual words aka tokens.\n",
    "    - Remove stop.words - frequently used pronouns and personal references.\n",
    "        - Top ten include: I, you, he, she, it, we, they, me, him, her\n",
    "    - Lemmatization - convert words to their root (optional).\n",
    "        - Lemmatization is a text normalization technique that reduces words to their base or dictionary form (lemma). Use to reduce inflected or derived words to their root form for better analysis and modeling outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, split, explode, length, lower, regexp_replace, count, row_number\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Clean and lowercase text, remove punctuation\n",
    "df_train_cleaned = df_train.withColumn(\"clean_text\", lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z0-9\\s]\", \"\")))\n",
    "\n",
    "# Tokenize into words then filter out empty strings after tokenization\n",
    "df_train_words = df_train_cleaned.withColumn(\"word\", explode(split(col(\"clean_text\"), r\"\\s+\"))).filter(col('word') != \"\")\n",
    "\n",
    "# Remove stop words\n",
    "df_train_filtered = df_train_words.filter(~col(\"word\").isin(stop_words))\n",
    "\n",
    "df_train_filtered.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a71be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 Analysis and Visualizations\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ---------- CHART 1: Most Frequent Word Lengths ----------\n",
    "# Get word frequency\n",
    "df_word_freq = df_train_filtered.groupBy(\"word\").agg(count(\"*\").alias(\"frequency\"))\n",
    "\n",
    "# Get top 30 most frequent words\n",
    "df_top_words = df_word_freq.orderBy(col(\"frequency\").desc()).limit(30)\n",
    "\n",
    "# Convert to pandas for sns\n",
    "pdf_top_words = df_top_words.toPandas()\n",
    "\n",
    "# Plot Chart 1\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=pdf_top_words, x=\"word\", y=\"frequency\", color=\"skyblue\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Top 30 Most Frequent Non-Stopwords\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- CHART 2: Most Frequent Word Lengths ----------\n",
    "# Get word lengths\n",
    "df_word_lengths = df_train_filtered.withColumn(\"length\", length(col(\"word\")))\n",
    "\n",
    "# Group by author and length, then count occurrences\n",
    "df_grouped = df_word_lengths.groupBy(\"author\", \"length\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Convert to pandas for sns\n",
    "pdf_word_lengths = df_grouped.toPandas()\n",
    "\n",
    "# Plot chart 2\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=pdf_word_lengths, x=\"length\", y=\"count\", hue=\"author\")\n",
    "plt.title(\"Most Frequent Word Lengths by Author (Excluding Stop Words)\")\n",
    "plt.xlabel(\"Word Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- CHART 3: Top 10 Longest Words per Author ----------\n",
    "# Group by author and word, get length of word\n",
    "df_longest = df_word_lengths.groupBy(\"author\", \"word\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"),\n",
    "    F.max(length(col('word'))).alias('length')\n",
    ")\n",
    "\n",
    "# Rank words by length within each author\n",
    "windowSpec = Window.partitionBy(\"author\").orderBy(col(\"length\").desc())\n",
    "\n",
    "# Get top 10 words per author\n",
    "df_top_longest = df_longest.withColumn(\"rank\", row_number().over(windowSpec)).filter(col(\"rank\") <= 10)\n",
    "\n",
    "# Convert to pandas for sns\n",
    "pdf_longest = df_top_longest.select(\"author\", \"word\", \"length\").toPandas()\n",
    "\n",
    "# Plot chart 3\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(data=pdf_longest, x=\"length\", y=\"word\", hue=\"author\")\n",
    "plt.title(\"Top 10 Longest Words per Author\")\n",
    "plt.xlabel(\"Word Length\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- CHART 4: count unique words by author ----------\n",
    "# Get unique words per author\n",
    "df_unique_words = df_train_filtered.select(\"author\", \"word\").distinct()\n",
    "\n",
    "# Count the unique words per author\n",
    "df_word_diversity = df_unique_words.groupBy(\"author\").count().withColumnRenamed(\"count\", \"unique_word_count\")\n",
    "\n",
    "# Convert to pandas for sns\n",
    "pdf_diversity = df_word_diversity.toPandas()\n",
    "\n",
    "# Define custom color palette \n",
    "palette = {\"EAP\": \"#4C72B0\", \"HPL\": \"#DD8452\", \"MWS\": \"#55A868\"}\n",
    "\n",
    "# Plot chart 4\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(data=pdf_diversity, x=\"author\", y=\"unique_word_count\", hue=\"author\", palette=palette, legend=False)\n",
    "plt.title(\"Word Diversity (Unique Words Used) per Author\")\n",
    "plt.xlabel(\"Author\")\n",
    "plt.ylabel(\"Unique Word Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5b4b8",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 2 - Feature Extraction\n",
    "1) Perform TFIDF to quantify word importance <href><https://en.wikipedia.org/wiki/Tf%E2%80%93idf></href>\n",
    "2) Normalize is scaling or standardizing the numerical features to a standard range or distribution\n",
    "    - In text mining, normalization vectorizes features with methods like TFIDF, a numerical measurement, to ensure a consistent scale\n",
    "    - It handles variations in the magnitude of feature values impacting machine-learning algorithm performance. Normalize the features to ensure a similar scale and prevent features with larger values from dominating the analysis or modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2848f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 - TFIDF and Normalization \n",
    "from pyspark.sql.functions import col, collect_list, struct, first\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "# Aggregate words into list per id, author (currently they are a single field per row) and retain text column\n",
    "df_grouped = df_train_filtered.groupBy(\"id\", \"author\").agg(\n",
    "    collect_list(\"word\").alias(\"words\"),\n",
    "    first('clean_text').alias('clean_text')\n",
    ")\n",
    "\n",
    "# Compute HashingTF\n",
    "hashingTF = HashingTF(inputCol='words', outputCol='tf', numFeatures=4096) # I selected 4096 for no particular reason, this can be tweaked.                           \n",
    "tf_data_train = hashingTF.transform(df_grouped)\n",
    "\n",
    "# Compute IDF\n",
    "idf = IDF(inputCol='tf', outputCol='tfidf', minDocFreq=3)\n",
    "idf_model_train = idf.fit(tf_data_train)\n",
    "tfidf_data_train = idf_model_train.transform(tf_data_train)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = Normalizer(inputCol='tfidf', outputCol='tfidf_norm', p=2.0)\n",
    "tfidf_data_train = normalizer.transform(tfidf_data_train)\n",
    "\n",
    "# Drop unneeded columns and show a few rows\n",
    "tfidf_data_train = tfidf_data_train.drop('tf', 'words')\n",
    "tfidf_data_train.select('tfidf').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01db475",
   "metadata": {},
   "source": [
    "The data has this structure: `[Vector length], [indicies], [tf-idf values]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 Visualizations (ex: Most Important Word By Author)\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Redo TFIDF: Need to use CountVectorizer here to retain the words for future analysis\n",
    "cv = CountVectorizer(inputCol='words', outputCol='tf_cv', minDF=3.0, vocabSize=4096)\n",
    "cv_model = cv.fit(tf_data_train)\n",
    "tf_cv_data = cv_model.transform(tf_data_train)\n",
    "idf_cv = IDF(minDocFreq=3, inputCol='tf_cv', outputCol='tfidf')\n",
    "idf_cv_model = idf.fit(tf_cv_data)\n",
    "tfidf_cv_data = idf_cv_model.transform(tf_cv_data).drop('tf_cv')\n",
    "tfidf_cv_data.cache()\n",
    "\n",
    "# Here I have to switch to using Pandas, as Spark would have timeout issues when trying to parse the TFIDF dataframe. Since this is just for visualization this should be fine\n",
    "tfidf_pandas = tfidf_cv_data.select('author', 'tfidf').toPandas()\n",
    "\n",
    "# Group the tfidf vectors per author\n",
    "top_words_per_author = defaultdict(list)\n",
    "for _, row in tfidf_pandas.iterrows():\n",
    "    author = row['author']\n",
    "    vector = row['tfidf']\n",
    "    for index, value in zip(vector.indices, vector.values):\n",
    "        top_words_per_author[author].append((index, value))\n",
    "\n",
    "# Now compute the average tfidf value per word per author\n",
    "avg_tfidf_per_author = {}\n",
    "vocab = cv_model.vocabulary\n",
    "for author, terms in top_words_per_author.items():\n",
    "    # For each term get the sum and counts\n",
    "    index_sums = defaultdict(lambda: {'sum': 0.0, 'count': 0})\n",
    "    for index, value in terms:\n",
    "        index_sums[index]['sum'] += value\n",
    "        index_sums[index]['count'] += 1\n",
    "    \n",
    "    # Now compute the averages\n",
    "    avg_tfidf = [(vocab[index], data['sum'] / data['count']) for index, data in index_sums.items()]\n",
    "\n",
    "    # Sort averages by descending average value (element 1 in avg_tfidf above) and get top 10 words\n",
    "    avg_tfidf = sorted(avg_tfidf, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # Assign them to the correct author\n",
    "    avg_tfidf_per_author[author] = pd.DataFrame(avg_tfidf, columns=['word', 'avg(value)'])\n",
    "\n",
    "# Plot the best words per author\n",
    "for author, df in avg_tfidf_per_author.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(df['word'], df['avg(value)'], color='lightblue')\n",
    "    plt.xlabel('Word')\n",
    "    plt.ylabel('Average TF-IDF')\n",
    "    plt.title(f'Top 10 Words for {author}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de907c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 3 - Machine Learning\n",
    "1) Perform train/test split\n",
    "2) Perform algorithmic analysis to assess and predict test labels\n",
    "    - Use as many algorithms as you need to get a good answer.\n",
    "    - Supervised: logistic regression, random forest, support vector machines, etc.\n",
    "    - Unsupervised: K-means, dimensionality reduction, PCA, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3 Solution (Due by Monday 7/21)\n",
    "# Each team member will do 2 algorithms of their choosing\n",
    "\n",
    "# Train test split for below\n",
    "train_data, test_data = tfidf_data_train.randomSplit([0.7, 0.3], seed=42)\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "print(f\"Training set size: {train_data.count()} rows\")\n",
    "print(f\"Test set size: {test_data.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aidan: Logistic Regression, Agglomerative Heirarchical Clustering\n",
    "\n",
    "# ---------- Logistic Regression ----------\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Convert author labels to numeric index\n",
    "indexer = StringIndexer(inputCol='author', outputCol='label')\n",
    "indexer_model = indexer.fit(train_data)\n",
    "indexed_train = indexer_model.transform(train_data)\n",
    "indexed_test = indexer_model.transform(test_data)\n",
    "indexed_train.cache()\n",
    "indexed_train.count() # Force the train data to cache\n",
    "\n",
    "# Define model\n",
    "lr = LogisticRegression(featuresCol='tfidf', labelCol='label', maxIter=1000)\n",
    "\n",
    "# Try to tune hyper params\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [1/c for c in [0.1, 1, 10]]) \\\n",
    "    .build()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3,\n",
    "                    parallelism=4)\n",
    "cv_model = cv.fit(indexed_train)\n",
    "lr_model = cv_model.bestModel\n",
    "\n",
    "# Predict test data and convert numerical labels back to authors\n",
    "lr_predictions = lr_model.transform(indexed_test)\n",
    "\n",
    "label_mapping = indexer_model.labels  # Get mapping of indices to author names (e.g., ['EAP', 'HPL', 'MWS'])\n",
    "lr_predictions = lr_predictions.withColumn(\"predicted_author\",\n",
    "    col(\"prediction\").cast(\"integer\").cast(\"string\"))  # Convert prediction to string\n",
    "lr_predictions = lr_predictions.replace(\n",
    "    to_replace={str(i): label_mapping[i] for i in range(len(label_mapping))},\n",
    "    subset=[\"predicted_author\"]\n",
    ")\n",
    "\n",
    "# Print top 5 predictions\n",
    "print(\"---------------- Logistic Regression Predictions ----------------\")\n",
    "lr_predictions.select(\"clean_text\", \"author\", \"label\", \"prediction\").show(5, truncate=False)\n",
    "\n",
    "# ---------- Agglomerative Heirarchical Clustering (Using Bisecting KMeans) ----------\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# Train the Bisecting KMeans model\n",
    "bkm = BisectingKMeans(featuresCol='tfidf', k=3, seed=42)\n",
    "bkm_model = bkm.fit(train_data)\n",
    "\n",
    "# Cluster predictions on train and test data\n",
    "bkm_train_predictions = bkm_model.transform(train_data)\n",
    "bkm_test_predictions = bkm_model.transform(test_data)\n",
    "\n",
    "# Show sample cluster assignments for training data\n",
    "print(\"\\n\\n---------------- Train Data Cluster Assignments ----------------\")\n",
    "bkm_train_predictions.select(\"id\", \"clean_text\", \"author\", \"prediction\").show(5, truncate=False)\n",
    "\n",
    "# Show sample cluster assignments for test data\n",
    "print(\"\\n\\n---------------- Test Data Cluster Assignments ----------------\")\n",
    "bkm_test_predictions.select(\"id\", \"clean_text\", \"prediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c872e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# pca = PCA(k=10, inputCol=\"tfidf_norm\", outputCol=\"tfidf_norm_pca\")\n",
    "\n",
    "# train_data_pca = pca.fit(train_data).transform(train_data)\n",
    "\n",
    "# I'd previously attempted to use a PCA, but now with the\n",
    "# hashingtf feature count being higher, I cannot run.\n",
    "\n",
    "kmeans = KMeans(k=3,featuresCol='tfidf_norm')\n",
    "\n",
    "k_means_model = kmeans.fit(train_data)\n",
    "\n",
    "k_means_result = k_means_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel: Neural network\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "layers = [hashingTF.getNumFeatures(), 50, 12, 6, 6, 3]\n",
    "#layers = [hashingTF.getNumFeatures(), 25, 6, 3, 3, 3]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nn_trainer = MultilayerPerceptronClassifier(maxIter=150, layers=layers,stepSize= 0.008, blockSize=200, seed=1234,featuresCol='tfidf_norm',labelCol='label')\n",
    "\n",
    "nn_pipeline = Pipeline(stages=[\n",
    "    indexer\n",
    "    ,nn_trainer\n",
    "])\n",
    "\n",
    "# Fit and transform using same pipeline\n",
    "nn_model = nn_pipeline.fit(train_data)\n",
    "nn_results = nn_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction',\n",
    "    metricName='accuracy'\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(nn_results)\n",
    "print(f'Training Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claudine:  MulticlassClassification and LDA\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Prepare Data\n",
    "data = tfidf_data_train.select(\"tfidf_norm\", \"author\")\n",
    "\n",
    "# Convert author names to label numbers\n",
    "label_indexer = StringIndexer(inputCol=\"author\", outputCol=\"label\")\n",
    "indexed_data = label_indexer.fit(data).transform(data)\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = indexed_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Supervised: Multilayer Perceptron\n",
    "input_size = train_data.select(\"tfidf_norm\").first()[0].size\n",
    "layers = [input_size, 128, 64, 3]  \n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(labelCol=\"label\", featuresCol=\"tfidf_norm\", maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "mlp_model = mlp.fit(train_data)\n",
    "mlp_predictions = mlp_model.transform(test_data)\n",
    "\n",
    "# Unsupervised: LDA\n",
    "lda = LDA(k=3, seed=42, featuresCol=\"tfidf_norm\")\n",
    "lda_model = lda.fit(tfidf_data_train)\n",
    "topics = lda_model.describeTopics(10)\n",
    "topics.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952215d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radhika: Random Forest, NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Train Random Forest Classifier Model\n",
    "rf = RandomForestClassifier(featuresCol='tfidf', labelCol='label', numTrees=4)\n",
    "rf_model = rf.fit(indexed_train)\n",
    "rf_predictions = rf_model.transform(indexed_test) # Predict test data and convert numerical labels back to authors\n",
    "print(\"rf_predictions\", rf_predictions.head())\n",
    "\n",
    "# Train Naive Bayes Classifier Model\n",
    "nb = NaiveBayes(featuresCol='tfidf', labelCol='label', modelType='multinomial') # Initialize Naive Bayes model\n",
    "nb_model = nb.fit(indexed_train) # Train the model\n",
    "nb_predictions = nb_model.transform(indexed_test) # Make predictions on the test data\n",
    "print(\"nb_predictions\", nb_predictions.head())\n",
    "\n",
    "# Print top 5 predictions\n",
    "#print(\"---------------- PCA Predictions ----------------\")\n",
    "#train_pca.select(\"pca_features\").show(5, truncate=False)\n",
    "#pca_predictions.select(\"clean_text\", \"author\", \"label\", \"prediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774667f2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 4 - Evaluation and Visualization\n",
    "1) Choose a metric strategy to assess algorithmic performance like accuracy, precision, recall, or F1 score\n",
    "2) Visualize confusion matrix, correlations, and similar\n",
    "3) Identify important features contributing to classification\n",
    "4) Write a 2-3 sentence minimum of findings, learnings, and what you would do next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee704b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4 Solution (Due by Monday 7/21)\n",
    "# Each team member will evaluate their models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2986ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aidan\n",
    "# ---------- Logistic Regression ----------\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "f1_score = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression F1 Score: {f1_score}\")\n",
    "\n",
    "# Get confusion matrix\n",
    "df_lr_predictions = lr_predictions.select('label', 'prediction').toPandas()\n",
    "conf_mat = confusion_matrix(df_lr_predictions['label'], df_lr_predictions['prediction'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping, yticklabels=label_mapping)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Author\")\n",
    "plt.ylabel(\"True Author\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- Agglomerative Heirarchical Clustering (Using Bisecting KMeans) ----------\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Calculate silhouette score\n",
    "evaluator = ClusteringEvaluator(featuresCol='tfidf', predictionCol='prediction')\n",
    "silhouette = evaluator.evaluate(bkm_train_predictions)\n",
    "print(f\"Bisecting K-Means Silhouette Score: {silhouette}\")\n",
    "\n",
    "# Map clusters to authors\n",
    "cluster_author_counts = bkm_train_predictions.groupBy(\"prediction\", \"author\").agg(count(\"*\").alias(\"count\"))\n",
    "cluster_author_counts.show()\n",
    "\n",
    "# Plot cluster sizes\n",
    "cluster_counts_pd = bkm_train_predictions.groupBy(\"prediction\").count().toPandas()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=cluster_counts_pd, x=\"prediction\", y=\"count\")\n",
    "plt.title(\"Cluster Sizes (Bisecting K-Means)\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.ylabel(\"Number of Texts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f93c53",
   "metadata": {},
   "source": [
    "#### Aidan - Analysis of Logistic Regression and Agglomerative Heirarchical Clustering (Bisecting K-Means)\n",
    "1) Logistic regression did a good job of predicting the test data with minimal hyper param tuning (only 3 params and 3 folds, so 9 models), but it still only achieved ~70% for its accuracy score\n",
    "    - Next I would dive into tuning this more in depth and perform a large cross validation with many different params and iteration counts\n",
    "2) Bisecting K-Means did not perform well. As indicated by it's silhouette score of ~-0.01, the clusters are not well defined and probably overlap significantly.\n",
    "    - I would not go any further with this algorithm for 2 reasons. It's unsupervised and would require a lot of preprocessing to get a tangible result. Since the data is labeled, a supervised algorithm would be a better fit here as seen by Logistic Regressions accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36824696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel\n",
    "# -----------------   K-means      ---------------------------\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "km_test_predictions = k_means_model.transform(test_data)\n",
    "\n",
    "# Calculate silhouette score\n",
    "evaluator = ClusteringEvaluator(featuresCol='tfidf_norm', predictionCol='prediction')\n",
    "silhouette = evaluator.evaluate(km_test_predictions)\n",
    "print(f\"K-Means Silhouette Score: {silhouette}\")\n",
    "\n",
    "# Map clusters to authors\n",
    "cluster_author_counts = km_test_predictions.groupBy(\"prediction\", \"author\").agg(count(\"*\").alias(\"count\"))\n",
    "cluster_author_counts.show()\n",
    "\n",
    "# Plot cluster sizes\n",
    "cluster_counts_pd = km_test_predictions.groupBy(\"prediction\").count().toPandas()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=cluster_counts_pd, x=\"prediction\", y=\"count\")\n",
    "plt.title(\"Cluster Sizes (K-Means)\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.ylabel(\"Number of Texts\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ----------------- Neural Network ---------------------------\n",
    "\n",
    "nn_test_predictions = nn_model.transform(test_data)\n",
    "\n",
    "nn_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "acc_score = nn_evaluator.evaluate(nn_test_predictions)\n",
    "print(f\"Neural Network Accuracy Score: {acc_score}\")\n",
    "\n",
    "# Get confusion matrix\n",
    "nn_test_predictions = nn_test_predictions.select('label', 'prediction').toPandas()\n",
    "nn_conf_mat = confusion_matrix(nn_test_predictions['label'], nn_test_predictions['prediction'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(nn_conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping, yticklabels=label_mapping)\n",
    "plt.title(\"Neural Network Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Author\")\n",
    "plt.ylabel(\"True Author\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b70d5",
   "metadata": {},
   "source": [
    "#### Daniel Lillard Analysis - K means, MultilayerPerceptronClassifier\n",
    "\n",
    "1. K means: This algorithm ended up doing very poorly, it grouped all the text into one single cluster, there would have to be some feature engineering done, I had tried to do a PCA, however we had to reduce the TF number of features. We would have to find a way to explode the differences between authors for this to be viable.\n",
    "\n",
    "2. MultilayerPerceptronClassifier: It seems that this is a neural network. I used Keras in a previous class and found it easier to work with then this, I suppose that Spark really does make things harder! I was able to get around a 70% accuracy with minimal effort, NN's are good function approximators and I know can *theoretically* solve for any function. Some feature engineering and hyper-parameter tuning should make this model viable, unlike k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ef0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claudine: Stage 4 Evaluation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# MLP Classifier\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(mlp_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(mlp_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(mlp_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(mlp_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(\"Multilayer Perceptron Performance:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_df = mlp_predictions.select(\"label\", \"prediction\").toPandas()\n",
    "conf_matrix = pd.crosstab(conf_matrix_df[\"label\"], conf_matrix_df[\"prediction\"], rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"MLP Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# LDA Topic Interpretation\n",
    "# Show top 10 term indices and term weights for each topic\n",
    "print(\"Top 10 Words per Topic from LDA Model:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "print(\"LDA Evaluation Metrics:\")\n",
    "print(\"Log Likelihood:\", lda_model.logLikelihood(tfidf_data_train))\n",
    "print(\"Perplexity:\", lda_model.logPerplexity(tfidf_data_train))\n",
    "\n",
    "# Convert topics to Pandas DataFrame for heatmap\n",
    "topic_words_df = topics.toPandas()\n",
    "\n",
    "# heatmap of term weights\n",
    "term_matrix = pd.DataFrame(topic_words_df['termWeights'].to_list(), \n",
    "                           index=[f\"Topic {i}\" for i in topic_words_df['topic'].tolist()])\n",
    "term_matrix.columns = [f\"Word {i+1}\" for i in range(term_matrix.shape[1])]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(term_matrix, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "plt.title(\"LDA Term Weights per Topic\")\n",
    "plt.ylabel(\"Topics\")\n",
    "plt.xlabel(\"Top Words\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78849738",
   "metadata": {},
   "source": [
    "#### Claudine - Neural Network and LDA\n",
    "I used accuracy, precision, recall, and F1 score to evaluate the Multilayer Perceptron, which hit over 71% accuracy. The confusion matrix showed good performance in classifying authors. For LDA, I used log likelihood and perplexity to assess topic quality and reviewed the top words per topic. TF-IDF weights helped identify the most influential words in classification and topic grouping. Next, I’d try tuning MLP hyperparameters and adding features like sentence length or punctuation to see if they improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radhika\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# ------------------------- Random Forest ----------------------\n",
    "accuracy = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(\"Random Forest Algorithm Performance Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# Get confusion matrix\n",
    "df_rf_predictions = rf_predictions.select('label', 'prediction').toPandas()\n",
    "conf_matx = confusion_matrix(df_rf_predictions['label'], df_rf_predictions['prediction'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matx, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping, yticklabels=label_mapping)\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Author\")\n",
    "plt.ylabel(\"True Author\")\n",
    "plt.show()\n",
    "\n",
    "# The confusion matrix for the random forest is very much skewed towards author Edgar Allen Poe. Also, the accuracy for this algorithm is <50%.\n",
    "# This indicates that this algorithm is not well-suited to this data, and I think this is because it is a classification problem.\n",
    "\n",
    "# ------------------------- Naive Bayes ----------------------\n",
    "accuracy = evaluator.evaluate(nb_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(nb_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(nb_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(nb_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(\"Naive Bayes Algorithm Performance Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# Get confusion matrix\n",
    "df_nb_predictions = nb_predictions.select('label', 'prediction').toPandas()\n",
    "conf_matx = confusion_matrix(df_nb_predictions['label'], df_nb_predictions['prediction'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matx, annot=True, fmt='d', cmap='Reds', xticklabels=label_mapping, yticklabels=label_mapping)\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Author\")\n",
    "plt.ylabel(\"True Author\")\n",
    "plt.show()\n",
    "\n",
    "# The confusion matrix for naive bayes, on the other hand, is very well balanced. Also, the accuracy for this algorithm is naturally\n",
    "# quite high, indicating that this algorithm is very well-suited to this data. I think this is also because it is a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540061d",
   "metadata": {},
   "source": [
    "#### Radhika - Random Forest and Naive-Bayes\n",
    "1) Random Forest: The confusion matrix for the random forest is very much skewed towards author Edgar Allen Poe. Also, the accuracy for this algorithm is <50%. This indicates that this algorithm is not well-suited to this data, and I think this is because it is a classification problem.\n",
    "\n",
    "2) Naive-Bayes: The confusion matrix for naive bayes, on the other hand, is very well balanced. Also, the accuracy for this algorithm is naturally quite high, indicating that this algorithm is very well-suited to this data. I think this is also because it is a classification problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
