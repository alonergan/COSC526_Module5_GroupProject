{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a903cc6",
   "metadata": {},
   "source": [
    "# Module 5 - Spooky Authorship Identification\n",
    "#### Group 13\n",
    "- Aidan Lonergan\n",
    "- Daniel Lillard\n",
    "- Radhika Garg\n",
    "- Claudine Uwiragiye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de4c9a6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Objective\n",
    "1) Accurately identify the author of the sentences in the test set\n",
    "2) Perform all work with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1cba3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 0 - Import Data\n",
    "1) Create a code notebook called: code_6_of_10_data_mine_group13.ipynb\n",
    "2) Load the dataset into Spark data objects and explore structure, size, and distribution of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0010f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 0 Solution\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Start spark session and load train and test data sets\n",
    "spark = SparkSession.builder.appName(\"Module_5_Project\").getOrCreate()\n",
    "df_train = spark.read.csv('./train.csv', header=True, inferSchema=True, quote='\"', escape='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c498c",
   "metadata": {},
   "source": [
    "##### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print size and descriptive statistics\n",
    "print(\"==== DataSet Shape ====\")\n",
    "print(f\"{len(df_train.columns)} columns\\n{df_train.count()} rows\\n\")\n",
    "\n",
    "print(\"==== DataSet Descriptive Statistics ====\")\n",
    "print(df_train.describe().show())\n",
    "\n",
    "print(\"\\n==== DataSet Unique Authors ====\")\n",
    "print(df_train.select('author').distinct().show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72e21a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 1 - Data Preparation (Exploratory data analysis and text mining pre-processing)\n",
    "1) Perform exploratory data analysis and create visualizations and tables as needed\n",
    "2) Text Preprocessing: perform tasks like tokenization and stopwords removal to clean text data\n",
    "    - Tokenize - split the text into individual words aka tokens.\n",
    "    - Remove stop.words - frequently used pronouns and personal references.\n",
    "        - Top ten include: I, you, he, she, it, we, they, me, him, her\n",
    "    - Lemmatization - convert words to their root (optional).\n",
    "        - Lemmatization is a text normalization technique that reduces words to their base or dictionary form (lemma). Use to reduce inflected or derived words to their root form for better analysis and modeling outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, split, explode, length, lower, regexp_replace, count, row_number\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Clean and lowercase text, remove punctuation\n",
    "df_train_cleaned = df_train.withColumn(\"clean_text\", lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z0-9\\s]\", \"\")))\n",
    "\n",
    "# Tokenize into words then filter out empty strings after tokenization\n",
    "df_train_words = df_train_cleaned.withColumn(\"word\", explode(split(col(\"clean_text\"), r\"\\s+\"))).filter(col('word') != \"\")\n",
    "\n",
    "# Remove stop words\n",
    "df_train_filtered = df_train_words.filter(~col(\"word\").isin(stop_words))\n",
    "\n",
    "df_train_filtered.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a71be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 Analysis and Visualizations\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ---------- CHART 1: Most Frequent Word Lengths ----------\n",
    "# Get word frequency\n",
    "df_word_freq = df_train_filtered.groupBy(\"word\").agg(count(\"*\").alias(\"frequency\"))\n",
    "\n",
    "# Get top 30 most frequent words\n",
    "df_top_words = df_word_freq.orderBy(col(\"frequency\").desc()).limit(30)\n",
    "\n",
    "# Convert to pandas for sns\n",
    "pdf_top_words = df_top_words.toPandas()\n",
    "\n",
    "# Plot Chart 1\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=pdf_top_words, x=\"word\", y=\"frequency\", color=\"skyblue\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Top 30 Most Frequent Non-Stopwords\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- CHART 2: Most Frequent Word Lengths ----------\n",
    "# Get word lengths\n",
    "df_word_lengths = df_train_filtered.withColumn(\"length\", length(col(\"word\")))\n",
    "\n",
    "# Group by author and length, then count occurrences\n",
    "df_grouped = df_word_lengths.groupBy(\"author\", \"length\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Convert to pandas for sns\n",
    "pdf_word_lengths = df_grouped.toPandas()\n",
    "\n",
    "# Plot chart 2\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=pdf_word_lengths, x=\"length\", y=\"count\", hue=\"author\")\n",
    "plt.title(\"Most Frequent Word Lengths by Author (Excluding Stop Words)\")\n",
    "plt.xlabel(\"Word Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- CHART 3: Top 10 Longest Words per Author ----------\n",
    "# Group by author and word, get length of word\n",
    "df_longest = df_word_lengths.groupBy(\"author\", \"word\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"),\n",
    "    F.max(length(col('word'))).alias('length')\n",
    ")\n",
    "\n",
    "# Rank words by length within each author\n",
    "windowSpec = Window.partitionBy(\"author\").orderBy(col(\"length\").desc())\n",
    "\n",
    "# Get top 10 words per author\n",
    "df_top_longest = df_longest.withColumn(\"rank\", row_number().over(windowSpec)).filter(col(\"rank\") <= 10)\n",
    "\n",
    "# Convert to pandas for sns\n",
    "pdf_longest = df_top_longest.select(\"author\", \"word\", \"length\").toPandas()\n",
    "\n",
    "# Plot chart 3\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(data=pdf_longest, x=\"length\", y=\"word\", hue=\"author\")\n",
    "plt.title(\"Top 10 Longest Words per Author\")\n",
    "plt.xlabel(\"Word Length\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- CHART 4: count unique words by author ----------\n",
    "# Get unique words per author\n",
    "df_unique_words = df_train_filtered.select(\"author\", \"word\").distinct()\n",
    "\n",
    "# Count the unique words per author\n",
    "df_word_diversity = df_unique_words.groupBy(\"author\").count().withColumnRenamed(\"count\", \"unique_word_count\")\n",
    "\n",
    "# Convert to pandas for sns\n",
    "pdf_diversity = df_word_diversity.toPandas()\n",
    "\n",
    "# Define custom color palette \n",
    "palette = {\"EAP\": \"#4C72B0\", \"HPL\": \"#DD8452\", \"MWS\": \"#55A868\"}\n",
    "\n",
    "# Plot chart 4\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(data=pdf_diversity, x=\"author\", y=\"unique_word_count\", hue=\"author\", palette=palette, legend=False)\n",
    "plt.title(\"Word Diversity (Unique Words Used) per Author\")\n",
    "plt.xlabel(\"Author\")\n",
    "plt.ylabel(\"Unique Word Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5b4b8",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 2 - Feature Extraction\n",
    "1) Perform TFIDF to quantify word importance <href><https://en.wikipedia.org/wiki/Tf%E2%80%93idf></href>\n",
    "2) Normalize is scaling or standardizing the numerical features to a standard range or distribution\n",
    "    - In text mining, normalization vectorizes features with methods like TFIDF, a numerical measurement, to ensure a consistent scale\n",
    "    - It handles variations in the magnitude of feature values impacting machine-learning algorithm performance. Normalize the features to ensure a similar scale and prevent features with larger values from dominating the analysis or modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2848f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 - TFIDF and Normalization \n",
    "from pyspark.sql.functions import col, collect_list, struct, first\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "# Aggregate words into list per id, author (currently they are a single field per row) and retain text column\n",
    "df_grouped = df_train_filtered.groupBy(\"id\", \"author\").agg(\n",
    "    collect_list(\"word\").alias(\"words\"),\n",
    "    first('clean_text').alias('clean_text')\n",
    ")\n",
    "\n",
    "# Compute HashingTF\n",
    "hashingTF = HashingTF(inputCol='words', outputCol='tf', numFeatures=4096) # I selected 4096 for no particular reason, this can be tweaked.                           \n",
    "tf_data_train = hashingTF.transform(df_grouped)\n",
    "\n",
    "# Compute IDF\n",
    "idf = IDF(inputCol='tf', outputCol='tfidf', minDocFreq=3)\n",
    "idf_model_train = idf.fit(tf_data_train)\n",
    "tfidf_data_train = idf_model_train.transform(tf_data_train)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = Normalizer(inputCol='tfidf', outputCol='tfidf_norm', p=2.0)\n",
    "tfidf_data_train = normalizer.transform(tfidf_data_train)\n",
    "\n",
    "# Drop unneeded columns and show a few rows\n",
    "tfidf_data_train = tfidf_data_train.drop('tf', 'words')\n",
    "tfidf_data_train.select('tfidf').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01db475",
   "metadata": {},
   "source": [
    "The data has this structure: `[Vector length], [indicies], [tf-idf values]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 Visualizations (ex: Most Important Word By Author)\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Redo TFIDF: Need to use CountVectorizer here to retain the words for future analysis\n",
    "cv = CountVectorizer(inputCol='words', outputCol='tf_cv', minDF=3.0, vocabSize=4096)\n",
    "cv_model = cv.fit(tf_data_train)\n",
    "tf_cv_data = cv_model.transform(tf_data_train)\n",
    "idf_cv = IDF(minDocFreq=3, inputCol='tf_cv', outputCol='tfidf')\n",
    "idf_cv_model = idf.fit(tf_cv_data)\n",
    "tfidf_cv_data = idf_cv_model.transform(tf_cv_data).drop('tf_cv')\n",
    "tfidf_cv_data.cache()\n",
    "\n",
    "# Here I have to switch to using Pandas, as Spark would have timeout issues when trying to parse the TFIDF dataframe. Since this is just for visualization this should be fine\n",
    "tfidf_pandas = tfidf_cv_data.select('author', 'tfidf').toPandas()\n",
    "\n",
    "# Group the tfidf vectors per author\n",
    "top_words_per_author = defaultdict(list)\n",
    "for _, row in tfidf_pandas.iterrows():\n",
    "    author = row['author']\n",
    "    vector = row['tfidf']\n",
    "    for index, value in zip(vector.indices, vector.values):\n",
    "        top_words_per_author[author].append((index, value))\n",
    "\n",
    "# Now compute the average tfidf value per word per author\n",
    "avg_tfidf_per_author = {}\n",
    "vocab = cv_model.vocabulary\n",
    "for author, terms in top_words_per_author.items():\n",
    "    # For each term get the sum and counts\n",
    "    index_sums = defaultdict(lambda: {'sum': 0.0, 'count': 0})\n",
    "    for index, value in terms:\n",
    "        index_sums[index]['sum'] += value\n",
    "        index_sums[index]['count'] += 1\n",
    "    \n",
    "    # Now compute the averages\n",
    "    avg_tfidf = [(vocab[index], data['sum'] / data['count']) for index, data in index_sums.items()]\n",
    "\n",
    "    # Sort averages by descending average value (element 1 in avg_tfidf above) and get top 10 words\n",
    "    avg_tfidf = sorted(avg_tfidf, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # Assign them to the correct author\n",
    "    avg_tfidf_per_author[author] = pd.DataFrame(avg_tfidf, columns=['word', 'avg(value)'])\n",
    "\n",
    "# Plot the best words per author\n",
    "for author, df in avg_tfidf_per_author.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(df['word'], df['avg(value)'], color='lightblue')\n",
    "    plt.xlabel('Word')\n",
    "    plt.ylabel('Average TF-IDF')\n",
    "    plt.title(f'Top 10 Words for {author}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de907c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 3 - Machine Learning\n",
    "1) Perform train/test split\n",
    "2) Perform algorithmic analysis to assess and predict test labels\n",
    "    - Use as many algorithms as you need to get a good answer.\n",
    "    - Supervised: logistic regression, random forest, support vector machines, etc.\n",
    "    - Unsupervised: K-means, dimensionality reduction, PCA, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3 Solution (Due by Monday 7/21)\n",
    "# Each team member will do 2 algorithms of their choosing\n",
    "\n",
    "# Train test split for below\n",
    "train_data, test_data = tfidf_data_train.randomSplit([0.7, 0.3], seed=42)\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "print(f\"Training set size: {train_data.count()} rows\")\n",
    "print(f\"Test set size: {test_data.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aidan: Logistic Regression, Agglomerative Heirarchical Clustering\n",
    "\n",
    "# ---------- Logistic Regression ----------\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Convert author labels to numeric index\n",
    "indexer = StringIndexer(inputCol='author', outputCol='label')\n",
    "indexer_model = indexer.fit(train_data)\n",
    "indexed_train = indexer_model.transform(train_data)\n",
    "indexed_test = indexer_model.transform(test_data)\n",
    "indexed_train.cache()\n",
    "indexed_train.count() # Force the train data to cache\n",
    "\n",
    "# Define model\n",
    "lr = LogisticRegression(featuresCol='tfidf', labelCol='label', maxIter=1000)\n",
    "\n",
    "# Try to tune hyper params\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [1/c for c in [0.1, 1, 10]]) \\\n",
    "    .build()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3,\n",
    "                    parallelism=4)\n",
    "cv_model = cv.fit(indexed_train)\n",
    "lr_model = cv_model.bestModel\n",
    "\n",
    "# Predict test data and convert numerical labels back to authors\n",
    "lr_predictions = lr_model.transform(indexed_test)\n",
    "\n",
    "label_mapping = indexer_model.labels  # Get mapping of indices to author names (e.g., ['EAP', 'HPL', 'MWS'])\n",
    "lr_predictions = lr_predictions.withColumn(\"predicted_author\",\n",
    "    col(\"prediction\").cast(\"integer\").cast(\"string\"))  # Convert prediction to string\n",
    "lr_predictions = lr_predictions.replace(\n",
    "    to_replace={str(i): label_mapping[i] for i in range(len(label_mapping))},\n",
    "    subset=[\"predicted_author\"]\n",
    ")\n",
    "\n",
    "# Print top 5 predictions\n",
    "print(\"---------------- Logistic Regression Predictions ----------------\")\n",
    "lr_predictions.select(\"clean_text\", \"author\", \"label\", \"prediction\").show(5, truncate=False)\n",
    "\n",
    "# ---------- Agglomerative Heirarchical Clustering (Using Bisecting KMeans) ----------\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# Train the Bisecting KMeans model\n",
    "bkm = BisectingKMeans(featuresCol='tfidf', k=3, seed=42)\n",
    "bkm_model = bkm.fit(train_data)\n",
    "\n",
    "# Cluster predictions on train and test data\n",
    "bkm_train_predictions = bkm_model.transform(train_data)\n",
    "bkm_test_predictions = bkm_model.transform(test_data)\n",
    "\n",
    "# Show sample cluster assignments for training data\n",
    "print(\"\\n\\n---------------- Train Data Cluster Assignments ----------------\")\n",
    "bkm_train_predictions.select(\"id\", \"clean_text\", \"author\", \"prediction\").show(5, truncate=False)\n",
    "\n",
    "# Show sample cluster assignments for test data\n",
    "print(\"\\n\\n---------------- Test Data Cluster Assignments ----------------\")\n",
    "bkm_test_predictions.select(\"id\", \"clean_text\", \"prediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c872e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# pca = PCA(k=10, inputCol=\"tfidf_norm\", outputCol=\"tfidf_norm_pca\")\n",
    "\n",
    "# train_data_pca = pca.fit(train_data).transform(train_data)\n",
    "\n",
    "# I'd previously attempted to use a PCA, but now with the\n",
    "# hashingtf feature count being higher, I cannot run.\n",
    "\n",
    "kmeans = KMeans(k=3,featuresCol='tfidf_norm')\n",
    "\n",
    "k_means_model = kmeans.fit(train_data)\n",
    "\n",
    "k_means_result = k_means_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel: Neural network\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "layers = [hashingTF.getNumFeatures(), 50, 12, 6, 6, 3]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nn_trainer = MultilayerPerceptronClassifier(maxIter=150, layers=layers,stepSize= 0.008, blockSize=200, seed=1234,featuresCol='tfidf_norm',labelCol='label')\n",
    "\n",
    "nn_pipeline = Pipeline(stages=[\n",
    "    indexer\n",
    "    ,nn_trainer\n",
    "])\n",
    "\n",
    "# Fit and transform using same pipeline\n",
    "nn_model = nn_pipeline.fit(train_data)\n",
    "nn_results = nn_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction',\n",
    "    metricName='accuracy'\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(nn_results)\n",
    "print(f'Training Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74320fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claudine: Decision Tree and LDA in PySpark\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Use normalized TFIDF and correct DataFrame\n",
    "data = tfidf_data_train.select(\"tfidf_norm\", \"author\")\n",
    "\n",
    "# Index labels\n",
    "label_indexer = StringIndexer(inputCol=\"author\", outputCol=\"label\")\n",
    "indexed_data = label_indexer.fit(data).transform(data)\n",
    "\n",
    "# Split\n",
    "train_data, test_data = indexed_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Decision Tree with maxDepth\n",
    "dt = DecisionTreeClassifier(featuresCol=\"tfidf_norm\", labelCol=\"label\", maxDepth=10)\n",
    "dt_model = dt.fit(train_data)\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# LDA Topic Modeling (still uses raw TFIDF)\n",
    "lda = LDA(k=3, seed=42, featuresCol=\"tfidf\")\n",
    "lda_model = lda.fit(tfidf_data_train)\n",
    "topics = lda_model.describeTopics(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claudine: Decision Tree (with CrossValidator) + LDA\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Prepare data\n",
    "data = tfidf_data_train.select(\"tfidf_norm\", \"author\")\n",
    "label_indexer = StringIndexer(inputCol=\"author\", outputCol=\"label\")\n",
    "indexed_data = label_indexer.fit(data).transform(data)\n",
    "\n",
    "# Split\n",
    "train_data, test_data = indexed_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Decision Tree setup\n",
    "dt = DecisionTreeClassifier(featuresCol=\"tfidf_norm\", labelCol=\"label\")\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(dt.maxDepth, [10, 15, 20])\n",
    "              .addGrid(dt.minInstancesPerNode, [1, 2])\n",
    "              .build())\n",
    "\n",
    "# Evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Cross-validator\n",
    "cv = CrossValidator(estimator=dt,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3)\n",
    "\n",
    "# Fit the best model\n",
    "cv_model = cv.fit(train_data)\n",
    "dt_best_model = cv_model.bestModel\n",
    "dt_predictions = dt_best_model.transform(test_data)\n",
    "\n",
    "# LDA (Unsupervised)\n",
    "lda = LDA(k=3, seed=42, featuresCol=\"tfidf_norm\")\n",
    "lda_model = lda.fit(tfidf_data_train)\n",
    "topics = lda_model.describeTopics(10)\n",
    "topics.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952215d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radhika: Random Forest, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774667f2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 4 - Evaluation and Visualization\n",
    "1) Choose a metric strategy to assess algorithmic performance like accuracy, precision, recall, or F1 score\n",
    "2) Visualize confusion matrix, correlations, and similar\n",
    "3) Identify important features contributing to classification\n",
    "4) Write a 2-3 sentence minimum of findings, learnings, and what you would do next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee704b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4 Solution (Due by Monday 7/21)\n",
    "# Each team member will evaluate their models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2986ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aidan\n",
    "# ---------- Logistic Regression ----------\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "f1_score = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression F1 Score: {f1_score}\")\n",
    "\n",
    "# Get confusion matrix\n",
    "df_lr_predictions = lr_predictions.select('label', 'prediction').toPandas()\n",
    "conf_mat = confusion_matrix(df_lr_predictions['label'], df_lr_predictions['prediction'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping, yticklabels=label_mapping)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Author\")\n",
    "plt.ylabel(\"True Author\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- Agglomerative Heirarchical Clustering (Using Bisecting KMeans) ----------\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Calculate silhouette score\n",
    "evaluator = ClusteringEvaluator(featuresCol='tfidf', predictionCol='prediction')\n",
    "silhouette = evaluator.evaluate(bkm_train_predictions)\n",
    "print(f\"Bisecting K-Means Silhouette Score: {silhouette}\")\n",
    "\n",
    "# Map clusters to authors\n",
    "cluster_author_counts = bkm_train_predictions.groupBy(\"prediction\", \"author\").agg(count(\"*\").alias(\"count\"))\n",
    "cluster_author_counts.show()\n",
    "\n",
    "# Plot cluster sizes\n",
    "cluster_counts_pd = bkm_train_predictions.groupBy(\"prediction\").count().toPandas()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=cluster_counts_pd, x=\"prediction\", y=\"count\")\n",
    "plt.title(\"Cluster Sizes (Bisecting K-Means)\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.ylabel(\"Number of Texts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f93c53",
   "metadata": {},
   "source": [
    "#### Aidan Analysis of Logistic Regression and Agglomerative Heirarchical Clustering (Bisecting K-Means)\n",
    "1) Logistic regression did a good job of predicting the test data with minimal hyper param tuning (only 3 params and 3 folds, so 9 models), but it still only achieved ~70% for its accuracy score\n",
    "    - Next I would dive into tuning this more in depth and perform a large cross validation with many different params and iteration counts\n",
    "2) Bisecting K-Means did not perform well. As indicated by it's silhouette score of ~-0.01, the clusters are not well defined and probably overlap significantly.\n",
    "    - I would not go any further with this algorithm for 2 reasons. It's unsupervised and would require a lot of preprocessing to get a tangible result. Since the data is labeled, a supervised algorithm would be a better fit here as seen by Logistic Regressions accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36824696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel\n",
    "# -----------------   K-means      ---------------------------\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "km_test_predictions = k_means_model.transform(test_data)\n",
    "\n",
    "# Calculate silhouette score\n",
    "evaluator = ClusteringEvaluator(featuresCol='tfidf_norm', predictionCol='prediction')\n",
    "silhouette = evaluator.evaluate(km_test_predictions)\n",
    "print(f\"K-Means Silhouette Score: {silhouette}\")\n",
    "\n",
    "# Map clusters to authors\n",
    "cluster_author_counts = km_test_predictions.groupBy(\"prediction\", \"author\").agg(count(\"*\").alias(\"count\"))\n",
    "cluster_author_counts.show()\n",
    "\n",
    "# Plot cluster sizes\n",
    "cluster_counts_pd = km_test_predictions.groupBy(\"prediction\").count().toPandas()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=cluster_counts_pd, x=\"prediction\", y=\"count\")\n",
    "plt.title(\"Cluster Sizes (K-Means)\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.ylabel(\"Number of Texts\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ----------------- Neural Network ---------------------------\n",
    "\n",
    "nn_test_predictions = nn_model.transform(test_data)\n",
    "\n",
    "nn_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "acc_score = nn_evaluator.evaluate(nn_test_predictions)\n",
    "print(f\"Neural Network Accuracy Score: {acc_score}\")\n",
    "\n",
    "# Get confusion matrix\n",
    "nn_test_predictions = nn_test_predictions.select('label', 'prediction').toPandas()\n",
    "nn_conf_mat = confusion_matrix(nn_test_predictions['label'], nn_test_predictions['prediction'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(nn_conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping, yticklabels=label_mapping)\n",
    "plt.title(\"Neural Network Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Author\")\n",
    "plt.ylabel(\"True Author\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ef0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claudine\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluation: Decision Tree\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(\"Decision Tree Performance:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix Plot\n",
    "conf_matrix_df = dt_predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "# Convert label numbers to string if needed (Optional)\n",
    "confusion_matrix = pd.crosstab(conf_matrix_df[\"label\"], conf_matrix_df[\"prediction\"], rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Decision Tree Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation: LDA Topics\n",
    "print(\"Top 10 Words per Topic from LDA Model:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Display top words for each topic using CountVectorizer's vocabulary\n",
    "vocab = cv_model.vocabulary  # This gives us the actual word list\n",
    "\n",
    "print(\"Top 10 Words per Topic from LDA Model:\")\n",
    "topic_indices = lda_model.describeTopics(10)\n",
    "topic_indices_local = topic_indices.limit(3).collect()\n",
    "\n",
    "for i, row in enumerate(topic_indices_local):\n",
    "    word_indices = row['termIndices']\n",
    "    words = [vocab[idx] for idx in word_indices]\n",
    "    print(f\"Topic {i}: {', '.join(words)}\")\n",
    "\n",
    "\n",
    "for i, words in enumerate(words):\n",
    "    print(f\"Topic {i}: {', '.join(words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3 â€“ Machine Learning\n",
    "# Claudine (Decision Tree & LDA)\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Use normalized TF-IDF vectors from Stage 2\n",
    "data = tfidf_data_train.select(\"tfidf_norm\", \"author\")\n",
    "\n",
    "# Encode author names to numeric labels\n",
    "label_indexer = StringIndexer(inputCol=\"author\", outputCol=\"label\")\n",
    "data_indexed = label_indexer.fit(data).transform(data)\n",
    "\n",
    "# Split dataset\n",
    "train_data, test_data = data_indexed.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# --- Decision Tree Classifier (Supervised) ---\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol=\"tfidf_norm\",\n",
    "    labelCol=\"label\",\n",
    "    maxDepth=18,               # try higher depth\n",
    "    minInstancesPerNode=2,     # allow splits with fewer samples\n",
    "    #impurity=\"gini\"            # default is fine, but you can also test \"entropy\"\n",
    ")\n",
    "\n",
    "dt_model = dt.fit(train_data)\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# --- LDA (Unsupervised Topic Modeling) ---\n",
    "lda = LDA(k=5, seed=42, featuresCol=\"tfidf_norm\")  # 5 topics for deeper separation\n",
    "lda_model = lda.fit(tfidf_data_train)\n",
    "\n",
    "# Show top 10 words per topic\n",
    "topics = lda_model.describeTopics(10)\n",
    "topics.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radhika"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
