{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654f0f06",
   "metadata": {},
   "source": [
    "# M6 Team Assignment: Spooky Authorship With Spark Part 2\n",
    "\n",
    "#### Group 13\n",
    "- Aidan Lonergan\n",
    "- Daniel Lillard\n",
    "- Radhika Garg\n",
    "- Claudine Uwiragiye\n",
    "\n",
    "## Objective\n",
    "- In this assignment, your team will improve your scores from the first Spooky Authorship assignment. Your goal should be to get at least a 80% accuracy. If you already have over 80% accuracy, aim to get 85% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f142fb90",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 0 - Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3501cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 0 Solution\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Start spark session and load train and test data sets\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module_5_Project\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.python.worker.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.pyspark.memory\", \"2g\") \\\n",
    "    .config(\"spark.rpc.io.connectionTimeout\", \"30s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_train = spark.read.csv('./train.csv', header=True, inferSchema=True, quote='\"', escape='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd86d2d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5a430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aflon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Preprocessing\n",
    "from pyspark.sql.functions import col, split, explode, lower, regexp_replace, udf, collect_list, first\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "# Get stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Clean and lowercase text, remove punctuation\n",
    "df_train_cleaned = df_train.withColumn(\"clean_text\", lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z0-9\\s]\", \"\")))\n",
    "\n",
    "# Tokenize into words then filter out empty strings after tokenization\n",
    "df_train_words = df_train_cleaned.withColumn(\"word\", explode(split(col(\"clean_text\"), r\"\\s+\"))).filter(col('word') != \"\")\n",
    "\n",
    "# Remove stop words\n",
    "df_train_filtered = df_train_words.filter(~col(\"word\").isin(stop_words))\n",
    "\n",
    "# Aggregate words into list per id, author (currently they are a single field per row) and retain text column\n",
    "df_grouped = df_train_filtered.groupBy(\"id\", \"author\").agg(\n",
    "    collect_list(\"word\").alias(\"words\"),\n",
    "    first('clean_text').alias('clean_text')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2115d8",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 2 - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2511d35",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o440.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 4.0 failed 1 times, most recent failure: Lost task 13.0 in stage 4.0 (TID 45) (DESKTOP-SHQ9QCJ.attlocal.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 37 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:266)\r\n\tat org.apache.spark.sql.classic.Dataset.materializedRdd$lzycompute(Dataset.scala:1578)\r\n\tat org.apache.spark.sql.classic.Dataset.materializedRdd(Dataset.scala:1576)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$rdd$1(Dataset.scala:1586)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2221)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\r\n\tat org.apache.spark.sql.classic.Dataset.rdd$lzycompute(Dataset.scala:1586)\r\n\tat org.apache.spark.sql.classic.Dataset.rdd(Dataset.scala:1584)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:90)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 37 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Compute IDF\u001b[39;00m\n\u001b[32m     10\u001b[39m idf = IDF(inputCol=\u001b[33m'\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m'\u001b[39m, outputCol=\u001b[33m'\u001b[39m\u001b[33mtfidf\u001b[39m\u001b[33m'\u001b[39m, minDocFreq=\u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m idf_model_train = \u001b[43midf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_data_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m tfidf_data_train = idf_model_train.transform(tf_data_train)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Normalize the data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aflon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aflon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\util.py:164\u001b[39m, in \u001b[36mtry_remote_fit.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aflon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:411\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aflon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:407\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aflon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aflon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aflon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o440.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 4.0 failed 1 times, most recent failure: Lost task 13.0 in stage 4.0 (TID 45) (DESKTOP-SHQ9QCJ.attlocal.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 37 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:266)\r\n\tat org.apache.spark.sql.classic.Dataset.materializedRdd$lzycompute(Dataset.scala:1578)\r\n\tat org.apache.spark.sql.classic.Dataset.materializedRdd(Dataset.scala:1576)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$rdd$1(Dataset.scala:1586)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2221)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\r\n\tat org.apache.spark.sql.classic.Dataset.rdd$lzycompute(Dataset.scala:1586)\r\n\tat org.apache.spark.sql.classic.Dataset.rdd(Dataset.scala:1584)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:90)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 37 more\r\n"
     ]
    }
   ],
   "source": [
    "# Stage 2 - TFIDF and Normalization \n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Compute HashingTF\n",
    "hashingTF = HashingTF(inputCol='words', outputCol='tf', numFeatures=4096) # I selected 4096 for no particular reason, this can be tweaked.                           \n",
    "tf_data_train = hashingTF.transform(df_grouped)\n",
    "\n",
    "# Compute IDF\n",
    "idf = IDF(inputCol='tf', outputCol='tfidf', minDocFreq=3)\n",
    "idf_model_train = idf.fit(tf_data_train)\n",
    "tfidf_data_train = idf_model_train.transform(tf_data_train)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = Normalizer(inputCol='tfidf', outputCol='tfidf_norm', p=2.0)\n",
    "tfidf_data_train = normalizer.transform(tfidf_data_train)\n",
    "\n",
    "# Drop unneeded columns and show a few rows\n",
    "tfidf_data_train = tfidf_data_train.drop('tf', 'words')\n",
    "tfidf_data_train.select('tfidf').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d43d1c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 3 - Machine Learning\n",
    "1) Perform train/test split\n",
    "2) Train neural network to achieve >85% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Train test split\n",
    "train_data, test_data = tfidf_data_train.randomSplit([0.7, 0.3], seed=42)\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "# Convert author labels to numeric index\n",
    "indexer = StringIndexer(inputCol='author', outputCol='label')\n",
    "indexer_model = indexer.fit(train_data)\n",
    "indexed_train = indexer_model.transform(train_data)\n",
    "indexed_test = indexer_model.transform(test_data)\n",
    "indexed_train.cache()\n",
    "indexed_train.count() # Force the train data to cache\n",
    "\n",
    "layers = [hashingTF.getNumFeatures(), 200, 100, 50, 3]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nn_trainer = MultilayerPerceptronClassifier(maxIter=150, layers=layers, stepSize= 0.03, blockSize=128, seed=1234,featuresCol='tfidf_norm',labelCol='label')\n",
    "\n",
    "nn_pipeline = Pipeline(stages=[\n",
    "    indexer\n",
    "    ,nn_trainer\n",
    "])\n",
    "\n",
    "# Fit and transform using same pipeline\n",
    "nn_model = nn_pipeline.fit(train_data)\n",
    "nn_results = nn_model.transform(train_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction',\n",
    "    metricName='accuracy'\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(nn_results)\n",
    "print(f'Training Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e390f0f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 4 - Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "label_mapping = indexer_model.labels  # Get mapping of indices to author names for analysis (e.g., ['EAP', 'HPL', 'MWS'])\n",
    "test_data_nn = test_data.drop('label')\n",
    "nn_test_predictions = nn_model.transform(test_data_nn)\n",
    "\n",
    "nn_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "acc_score = nn_evaluator.evaluate(nn_test_predictions)\n",
    "print(f\"Neural Network Accuracy Score: {acc_score}\")\n",
    "\n",
    "# Get confusion matrix\n",
    "nn_test_predictions = nn_test_predictions.select('label', 'prediction').toPandas()\n",
    "nn_conf_mat = confusion_matrix(nn_test_predictions['label'], nn_test_predictions['prediction'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(nn_conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping, yticklabels=label_mapping)\n",
    "plt.title(\"Neural Network Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Author\")\n",
    "plt.ylabel(\"True Author\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
