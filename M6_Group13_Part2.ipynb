{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654f0f06",
   "metadata": {},
   "source": [
    "# M6 Team Assignment: Spooky Authorship With Spark Part 2\n",
    "\n",
    "#### Group 13\n",
    "- Aidan Lonergan\n",
    "- Daniel Lillard\n",
    "- Radhika Garg\n",
    "- Claudine Uwiragiye\n",
    "\n",
    "## Objective\n",
    "- In this assignment, your team will improve your scores from the first Spooky Authorship assignment. Your goal should be to get at least a 80% accuracy. If you already have over 80% accuracy, aim to get 85% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f142fb90",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 0 - Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3501cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 0 Solution\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Start spark session and load train and test data sets\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module_5_Project\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"20g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .config(\"spark.python.worker.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.pyspark.memory\", \"2g\") \\\n",
    "    .config(\"spark.rpc.io.connectionTimeout\", \"30s\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .config(\"spark.task.cpus\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_train = spark.read.csv('./train.csv', header=True, inferSchema=True, quote='\"', escape='\"')\n",
    "\n",
    "# Optional: Subsample dataset for faster testing (comment out for full dataset)\n",
    "#df_train = df_train.limit(10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd86d2d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d5a430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aflon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Preprocessing\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.functions import col, split, explode, lower, regexp_replace, regexp_count, collect_list, first, length, array_union\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "# Get stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add punctuation count before removing it (Cannot use regexp_count here even when passing in a raw string...)\n",
    "df_train = df_train.withColumn(\"comma_count\", length(col(\"text\")) - length(regexp_replace(col(\"text\"), \",\", \"\")))\n",
    "\n",
    "# Clean and lowercase text, remove punctuation\n",
    "df_train_cleaned = df_train.withColumn(\"clean_text\", lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z0-9\\s]\", \"\")))\n",
    "\n",
    "# Tokenize into words then filter out empty strings after tokenization\n",
    "df_train_words = df_train_cleaned.withColumn(\"word\", explode(split(col(\"clean_text\"), r\"\\s+\"))).filter(col('word') != \"\")\n",
    "\n",
    "# Remove stop words\n",
    "df_train_filtered = df_train_words.filter(~col(\"word\").isin(stop_words))\n",
    "\n",
    "# Aggregate words into list per id, author (currently they are a single field per row) and retain text column\n",
    "df_grouped = df_train_filtered.groupBy(\"id\", \"author\").agg(\n",
    "    collect_list(\"word\").alias(\"words\"),\n",
    "    first(\"clean_text\").alias(\"clean_text\"),\n",
    "    first(\"comma_count\").alias(\"comma_count\")\n",
    ")\n",
    "df_grouped = df_grouped.repartition(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2115d8",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 2 - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2511d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, StandardScaler, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Train-test split on grouped data (before feature extraction)\n",
    "train_data_raw, test_data_raw = df_grouped.randomSplit([0.8, 0.2], seed=42)\n",
    "train_data_raw = train_data_raw.repartition(16)\n",
    "test_data_raw = test_data_raw.repartition(16)\n",
    "train_data_raw.cache()\n",
    "test_data_raw.cache()\n",
    "\n",
    "# Define stages for pipeline\n",
    "hashingTF = HashingTF(inputCol='words', outputCol='tf', numFeatures=4096)\n",
    "idf = IDF(inputCol='tf', outputCol='tfidf', minDocFreq=5)\n",
    "scaler = StandardScaler(inputCol='tfidf', outputCol='features', withMean=True, withStd=True)\n",
    "#assembler = VectorAssembler(inputCols=['tfidf_norm', 'comma_count', 'sentance_length'], outputCol='features')\n",
    "indexer = StringIndexer(inputCol='author', outputCol='label')\n",
    "\n",
    "# Define MLP trainer\n",
    "nn_trainer = MultilayerPerceptronClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    solver=\"l-bfgs\",\n",
    "    tol=1e-6,\n",
    "    maxIter=200,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Define pipeline with all stages\n",
    "nn_pipeline = Pipeline(stages=[hashingTF, idf, scaler, indexer, nn_trainer])\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction',\n",
    "    metricName='accuracy'\n",
    ")\n",
    "\n",
    "# Determine number of classes dynamically\n",
    "indexer_model = indexer.fit(train_data_raw)\n",
    "num_classes = len(indexer_model.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d43d1c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 3 - Machine Learning\n",
    "1) Perform train/test split\n",
    "2) Train neural network to achieve >85% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e16eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.9998092816274634\n",
      "Test  Acc: 0.718148725949038\n",
      "Best Params: {'numFeatures': 8192, 'layers': [8192, 300, 150, 75, 3], 'stepSize': 0.01, 'maxIter': 100, 'blockSize': 128}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "# Define parameter grid for cross-validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(hashingTF.numFeatures, [8192])\n",
    "    .addGrid(nn_trainer.layers, [\n",
    "        [8192, 300, 150, 75, 3]\n",
    "    ])\n",
    "    .addGrid(nn_trainer.stepSize, [0.01])\n",
    "    .addGrid(nn_trainer.maxIter, [100])\n",
    "    .addGrid(nn_trainer.blockSize, [128])\n",
    "    .build())\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=nn_pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,\n",
    "    parallelism=8\n",
    ")\n",
    "\n",
    "# 4. Fit & evaluate\n",
    "tvs_model = tvs.fit(train_data_raw)\n",
    "best_model = tvs_model.bestModel\n",
    "\n",
    "print(\"Train Acc:\", evaluator.evaluate(best_model.transform(train_data_raw)))\n",
    "print(\"Test  Acc:\", evaluator.evaluate(best_model.transform(test_data_raw)))\n",
    "print(\"Best Params:\", {\n",
    "    \"numFeatures\": best_model.stages[0].getNumFeatures(),\n",
    "    \"layers\":       best_model.stages[-1].getLayers(),\n",
    "    \"stepSize\":     best_model.stages[-1].getStepSize(),\n",
    "    \"maxIter\":      best_model.stages[-1].getMaxIter(),\n",
    "    \"blockSize\":    best_model.stages[-1].getBlockSize()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e390f0f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stage 4 - Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "label_mapping = indexer_model.labels  # Get mapping of indices to author names for analysis (e.g., ['EAP', 'HPL', 'MWS'])\n",
    "test_data_nn = test_data.drop('label')\n",
    "nn_test_predictions = nn_model.transform(test_data_nn)\n",
    "\n",
    "nn_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "acc_score = nn_evaluator.evaluate(nn_test_predictions)\n",
    "print(f\"Neural Network Accuracy Score: {acc_score}\")\n",
    "\n",
    "# Get confusion matrix\n",
    "nn_test_predictions = nn_test_predictions.select('label', 'prediction').toPandas()\n",
    "nn_conf_mat = confusion_matrix(nn_test_predictions['label'], nn_test_predictions['prediction'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(nn_conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping, yticklabels=label_mapping)\n",
    "plt.title(\"Neural Network Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Author\")\n",
    "plt.ylabel(\"True Author\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
